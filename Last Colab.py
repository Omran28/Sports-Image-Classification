# -*- coding: utf-8 -*-
"""Neural 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16McpFkpdE-qZDAIJgfNT0VytCRYz9a5h

**Imports**
"""

import os
import numpy as np
import tensorflow as tf
import cv2
import glob
import csv
import keras
from random import shuffle
from numpy import expand_dims
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.framework import ops
import torch.nn as nn
import tflearn
from tflearn.layers.conv import conv_2d, max_pool_2d, avg_pool_2d
from keras import Input
from keras.layers import Conv2D, MaxPool2D, AveragePooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, \
    concatenate
from tflearn.layers.core import input_data, dropout, fully_connected
from tflearn.layers.estimator import regression
import keras.backend as K
import tensorflow as tf
from keras.datasets import cifar10
from keras.models import Model
import math
from keras.optimizers import SGD
from keras.callbacks import LearningRateScheduler

"""**Augmentation methods**"""


def Horizontal_Flip(img):
    Change_Dimension = expand_dims(img, 0)
    Flipping = ImageDataGenerator(horizontal_flip=True, fill_mode='nearest', )
    it = Flipping.flow(Change_Dimension, batch_size=1)
    Next_it = it.next()
    Flipped_Image = Next_it[0].astype('uint8')
    return Flipped_Image


def Change_Brightness(img):
    Change_Dimension = expand_dims(img, 0)
    New_Brightness = ImageDataGenerator(brightness_range=[0.4, 0.4])
    it = New_Brightness.flow(Change_Dimension, batch_size=1)
    Next_it = it.next()
    Image = Next_it[0].astype('uint8')
    return Image


def Rotation(img):
    Change_Dimension = expand_dims(img, 0)
    Rotate = ImageDataGenerator(rotation_range=50, fill_mode='nearest')
    it = Rotate.flow(Change_Dimension, batch_size=1)
    Next_it = it.next()
    Rotated_Image = Next_it[0].astype('uint8')
    return Rotated_Image


def Width_Shift(img):
    Change_Dimension = expand_dims(img, 0)
    Shifting = ImageDataGenerator(width_shift_range=0.4, fill_mode='nearest')
    it = Shifting.flow(Change_Dimension, batch_size=1)
    Next_it = it.next()
    Shifted_Image = Next_it[0].astype('uint8')
    return Shifted_Image


def Height_Shift(img):
    Change_Dimension = expand_dims(img, 0)
    Shifting = ImageDataGenerator(height_shift_range=0.4, fill_mode='nearest')
    it = Shifting.flow(Change_Dimension, batch_size=1)
    Next_it = it.next()
    Shifted_Image = Next_it[0].astype('uint8')
    return Shifted_Image


def Zoom(img):
    Change_Dimension = expand_dims(img, 0)
    Zooming = ImageDataGenerator(zoom_range=0.5)
    it = Zooming.flow(Change_Dimension, batch_size=1)
    Next_it = it.next()
    Zoomed_Image = Next_it[0].astype('uint8')
    return Zoomed_Image


"""**Min-Max Normalization**"""


def rescale(img, min, max):
    img = (img - img.min()) / float(img.max() - img.min())
    img = min + img * (max - min)
    return img


"""**Creating Train and Validation data**"""


def create_train_data(b, f, r, s, t, y):
    b, B_label = augmentation_fun(b, [1, 0, 0, 0, 0, 0])
    f, F_label = augmentation_fun(f, [0, 1, 0, 0, 0, 0])
    r, R_label = augmentation_fun(r, [0, 0, 1, 0, 0, 0])
    s, S_label = augmentation_fun(s, [0, 0, 0, 1, 0, 0])
    t, T_label = augmentation_fun(t, [0, 0, 0, 0, 1, 0])
    y, Y_label = augmentation_fun(y, [0, 0, 0, 0, 0, 1])

    # Training data
    training_data = []
    train = b[0:592] + f[0:592] + r[0:592] + s[0:592] + t[0:592] + y[0:592]
    train_labels = B_label[0:592] + F_label[0:592] + R_label[0:592] + S_label[0:592] + T_label[0:592] + Y_label[0:592]

    # validation data
    Validation_data = []
    validation = b[592:740] + f[592:740] + r[592:740] + s[592:740] + t[592:740] + y[592:740]
    validation_labels = B_label[0:148] + F_label[0:148] + R_label[0:148] + S_label[0:148] + T_label[0:148] + Y_label[
                                                                                                             0:148]

    for i in range(len(train)):
        img = cv2.resize(train[i], (size, size))
        img = rescale(img, 0, 1)
        training_data.append([img, np.array(train_labels[i])])
        if i < 888:
            img = cv2.resize(validation[i], (size, size))
            img = rescale(img, 0, 1)
            Validation_data.append([img, np.array(validation_labels[i])])

    shuffle(training_data)
    shuffle(Validation_data)

    # np.save('training_data.npy', training_data)
    # np.save('validation_data.npy', Validation_data)
    return training_data, Validation_data


def augmentation_fun(images, label):
    new_images = []
    images_label = []
    for img in images:
        # original image
        images_label.append(label)

        # flip left to right
        L_R_flipped = Horizontal_Flip(img)
        new_images.append(L_R_flipped)
        images_label.append(label)

        # rotation
        # rotated = Rotation(img)
        # new_images.append(rotated)
        # images_label.append(label)

        # zoom
        zoomed = Zoom(img)
        new_images.append(zoomed)
        images_label.append(label)

        # change the brightness
        # brightened = Change_Brightness(img)
        # new_images.append(brightened)
        # images_label.append(label)

        # combination of flip L to R & Brightness
        brightened_Flipped = Change_Brightness(L_R_flipped)
        new_images.append(brightened_Flipped)
        images_label.append(label)

    images += new_images
    shuffle(images)
    return images, images_label


"""**Model Architecture**"""

kernel_init = keras.initializers.glorot_uniform()
bias_init = keras.initializers.Constant(value=0.2)


def inception_module(x,
                     filters_1x1,
                     filters_3x3_reduce,
                     filters_3x3,
                     filters_5x5_reduce,
                     filters_5x5,
                     filters_pool_proj,
                     name=None):
    conv_1x1 = Conv2D(filters_1x1, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init,
                      bias_initializer=bias_init)(x)

    conv_3x3 = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init,
                      bias_initializer=bias_init)(x)
    conv_3x3 = Conv2D(filters_3x3, (3, 3), padding='same', activation='relu', kernel_initializer=kernel_init,
                      bias_initializer=bias_init)(conv_3x3)

    conv_5x5 = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init,
                      bias_initializer=bias_init)(x)
    conv_5x5 = Conv2D(filters_5x5, (5, 5), padding='same', activation='relu', kernel_initializer=kernel_init,
                      bias_initializer=bias_init)(conv_5x5)

    pool_proj = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)
    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init,
                       bias_initializer=bias_init)(pool_proj)

    output = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)

    return output


def Model_Architecture():
    input_layer = Input(shape=(100, 100, 3))
    x = Conv2D(64, (7, 7), padding='same', strides=(2, 2), activation='relu', name='conv_1_7x7/2',
               kernel_initializer=kernel_init, bias_initializer=bias_init)(input_layer)
    x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_1_3x3/2')(x)
    x = Conv2D(64, (1, 1), padding='same', strides=(1, 1), activation='relu', name='conv_2a_3x3/1')(x)
    x = Conv2D(192, (3, 3), padding='same', strides=(1, 1), activation='relu', name='conv_2b_3x3/1')(x)
    x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_2_3x3/2')(x)

    x = inception_module(x,
                         filters_1x1=64,
                         filters_3x3_reduce=96,
                         filters_3x3=128,
                         filters_5x5_reduce=16,
                         filters_5x5=32,
                         filters_pool_proj=32,
                         name='inception_3a')

    x = inception_module(x,
                         filters_1x1=128,
                         filters_3x3_reduce=128,
                         filters_3x3=192,
                         filters_5x5_reduce=32,
                         filters_5x5=96,
                         filters_pool_proj=64,
                         name='inception_3b')

    x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_3_3x3/2')(x)

    x = inception_module(x,
                         filters_1x1=192,
                         filters_3x3_reduce=96,
                         filters_3x3=208,
                         filters_5x5_reduce=16,
                         filters_5x5=48,
                         filters_pool_proj=64,
                         name='inception_4a')

    x1 = AveragePooling2D((5, 5), strides=3)(x)
    x1 = Conv2D(128, (1, 1), padding='same', activation='relu')(x1)
    x1 = Flatten()(x1)
    x1 = Dense(1024, activation='relu')(x1)
    x1 = Dropout(0.7)(x1)
    x1 = Dense(6, activation='softmax', name='auxilliary_output_1')(x1)

    x = inception_module(x,
                         filters_1x1=160,
                         filters_3x3_reduce=112,
                         filters_3x3=224,
                         filters_5x5_reduce=24,
                         filters_5x5=64,
                         filters_pool_proj=64,
                         name='inception_4b')

    x = inception_module(x,
                         filters_1x1=128,
                         filters_3x3_reduce=128,
                         filters_3x3=256,
                         filters_5x5_reduce=24,
                         filters_5x5=64,
                         filters_pool_proj=64,
                         name='inception_4c')

    x = inception_module(x,
                         filters_1x1=112,
                         filters_3x3_reduce=144,
                         filters_3x3=288,
                         filters_5x5_reduce=32,
                         filters_5x5=64,
                         filters_pool_proj=64,
                         name='inception_4d')

    x2 = AveragePooling2D((5, 5), strides=3)(x)
    x2 = Conv2D(128, (1, 1), padding='same', activation='relu')(x2)
    x2 = Flatten()(x2)
    x2 = Dense(1024, activation='relu')(x2)
    x2 = Dropout(0.7)(x2)
    x2 = Dense(6, activation='softmax', name='auxilliary_output_2')(x2)

    x = inception_module(x,
                         filters_1x1=256,
                         filters_3x3_reduce=160,
                         filters_3x3=320,
                         filters_5x5_reduce=32,
                         filters_5x5=128,
                         filters_pool_proj=128,
                         name='inception_4e')

    x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_4_3x3/2')(x)

    x = inception_module(x,
                         filters_1x1=256,
                         filters_3x3_reduce=160,
                         filters_3x3=320,
                         filters_5x5_reduce=32,
                         filters_5x5=128,
                         filters_pool_proj=128,
                         name='inception_5a')

    x = inception_module(x,
                         filters_1x1=384,
                         filters_3x3_reduce=192,
                         filters_3x3=384,
                         filters_5x5_reduce=48,
                         filters_5x5=128,
                         filters_pool_proj=128,
                         name='inception_5b')

    x = GlobalAveragePooling2D(name='avg_pool_5_3x3/1')(x)

    x = Dropout(0.4)(x)

    x = Dense(6, activation='softmax', name='output')(x)
    model = Model(input_layer, [x, x1, x2], name='inception_v1')
    return model


epochs = 15
initial_lrate = 0.01


def decay(epoch, steps=100):
    initial_lrate = 0.01
    drop = 0.96
    epochs_drop = 8
    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))
    return lrate


sgd = SGD(lr=initial_lrate, momentum=0.9, nesterov=False)

lr_sc = LearningRateScheduler(decay, verbose=1)
model = Model_Architecture()
model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'],
              loss_weights=[1, 0.3, 0.3], optimizer=sgd, metrics=['accuracy'])

"""**Model Parameters**"""

size = 100
LR = 0.001
MODEL_NAME = "Sports Classification"

"""**Load Train Data**"""

train_images_path = glob.glob("/content/Train/*.*")

# Read train images and separate each sport
B = []  # 196
F = []  # 400
R = []  # 202
S = []  # 240
T = []  # 185
Y = []  # 458
for img in train_images_path:
    if img[15] == 'B':
        img1 = cv2.imread(img)
        # img1 = cv2.resize(img1, (size, size))
        # img1 = rescale(img1, 0, 1)
        B.append(img1)
    elif img[15] == 'F':
        img1 = cv2.imread(img)
        F.append(img1)
    elif img[15] == 'R':
        img1 = cv2.imread(img)
        R.append(img1)
    elif img[15] == 'S':
        img1 = cv2.imread(img)
        S.append(img1)
    elif img[15] == 'T':
        img1 = cv2.imread(img)
        T.append(img1)
    elif img[15] == 'Y':
        img1 = cv2.imread(img)
        Y.append(img1)

"""**Load Test Data**"""

test_images_path = glob.glob("/content/Test/*.*")

if os.path.exists('/content/testing_data.npy'):
    test_images = np.load('/content/testing_data.npy', allow_pickle=True)
else:
    # reading and resizing
    test_images = []
    for image in test_images_path:
        img = cv2.imread(image)
        img = cv2.resize(img, (size, size))
        test_images.append(img)
    np.save('testing_data.npy', test_images)

"""**Split data into Train and Test**"""

if os.path.exists('/content/training_data.npy'):
    training_data = np.load('/content/training_data.npy', allow_pickle=True)
    validation_data = np.load('/content/validation_data.npy', allow_pickle=True)
else:
    training_data, validation_data = create_train_data(B, F, R, S, T, Y)
    np.save('training_data.npy', training_data)
    np.save('validation_data.npy', validation_data)

# Training data
x_train = np.array([i[0] for i in training_data]).reshape(-1, size, size, 3)
y_train = [i[1] for i in training_data]

# Validation data
x_validate = np.array([i[0] for i in validation_data]).reshape(-1, size, size, 3)
y_validate = [i[1] for i in validation_data]

"""**Training the Model**"""

# # Training Model
# model = Model_Architecture()
# if os.path.exists('model.tfl.meta'):
#     model.load('./model.tfl')
# else:
#     model.fit({'input': x_train}, {'targets': y_train}, n_epoch=10,
#               validation_set=({'input': x_validate}, {'targets': y_validate}),
#               snapshot_step=500, show_metric=True, run_id=MODEL_NAME)
#     # model.save('model.tfl')
model = model.fit(x_train, [y_train, y_train, y_train],
                  validation_data=(x_validate, [y_validate, y_validate, y_validate]), epochs=epochs, batch_size=256,
                  callbacks=[lr_sc])

"""**Testing**"""

# Testing
with open('sub2.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['image_name', 'label'])
    for i in range(len(test_images)):
        image = test_images[i].reshape(-1, size, size, 3)
        prediction = model.predict(image)[0]
        max = 0.0
        index = 0
        for j in range(len(prediction)):
            if prediction[j] > max:
                max = prediction[j]
                index = j
        img_name = test_images_path[i].split("/")[-1]
        writer.writerow([str(img_name), index])
#         print(f"basketball: {prediction[0]}, football: {prediction[1]}, Rowing: {prediction[2]} ,"
#               f"swimming: {prediction[3]}, "f"tennis: {prediction[4]}, yoga: {prediction[5]}")
